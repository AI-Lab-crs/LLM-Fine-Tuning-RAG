{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama-2 7B, a model fine-tuned for generating text & chatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installations\n",
    "- Hugging Face Transformers: Provides us with a straightforward way to use pre-trained models.\n",
    "- PyTorch: Serves as the backbone for deep learning operations.\n",
    "- Accelerate: Optimizes PyTorch operations, especially on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "To load our desired model, meta-llama/Llama-2-7b-chat-hf, we first need to authenticate ourselves on Hugging Face. This ensures we have the correct permissions to fetch the model.\n",
    "\n",
    "- Gain access to the model on Hugging Face.\n",
    "- Use the Hugging Face CLI to login and verify your authentication status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login\n",
    "!huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Model & Tokenizer\n",
    "preparing our session by loading both the Llama model and its associated tokenizer.\n",
    "\n",
    "The tokenizer will help in converting our text prompts into a format that the model can understand and process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\" # meta-llama/Llama-2-7b-hf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Llama Pipeline\n",
    "We'll set up a pipeline for text generation.\n",
    "\n",
    "This pipeline simplifies the process of feeding prompts to our model and receiving generated text as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",  # LLM task\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Responses\n",
    "With everything set up, let's see how Llama responds to some sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama_response(prompt: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate a response from the Llama model.\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The user's input/question for the model.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the model's response.\n",
    "    \"\"\"\n",
    "    sequences = llama_pipeline(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=256,\n",
    "    )\n",
    "    print(\"Chatbot:\", sequences[0]['generated_text'])\n",
    "\n",
    "\n",
    "\n",
    "prompt = 'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n'\n",
    "get_llama_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"I'm a programmer and Python is my favorite language because of it's simple syntax and variety of applications I can build with it.\\\n",
    "Based on that, what language should I learn next?\\\n",
    "Give me 5 recommendations\"\"\"\n",
    "get_llama_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Tell me about Llama and Hugging face?\\n'\n",
    "get_llama_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make it conversational\n",
    "Let's create an interactive chat loop, where you can converse with the Llama model.\n",
    "\n",
    "Type your questions or comments, and see how the model responds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"bye\", \"quit\", \"exit\"]:\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    get_llama_response(user_input)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
